# ğŸ¤ AI Voice Assistant Chatbot

A sophisticated AI-powered voice assistant built with Streamlit that enables natural voice conversations with AI. The assistant listens to your voice, transcribes it, processes your query using a large language model, and responds back with a synthesized voice.

## âœ¨ Features

- ğŸ™ï¸ **Voice Recording**: Record your questions using a simple microphone interface
- ğŸ“ **Speech-to-Text**: Powered by Groq's Whisper model for accurate transcription
- ğŸ¤– **AI Processing**: Uses Ollama's DeepSeek model for intelligent responses
- ğŸ”Š **Text-to-Speech**: Natural-sounding voice responses using Edge TTS
- ğŸ’¬ **Conversation History**: Track all your interactions with timestamps
- ğŸ¨ **User-Friendly Interface**: Clean and intuitive Streamlit UI
- âš¡ **Real-time Processing**: Quick response times with async operations

## ğŸ”„ Processing Pipeline

```
1. ğŸ™ï¸ Audio Recording
   â†“
2. ğŸ“ Speech-to-Text (Whisper)
   â†“
3. ğŸ¤– LLM Processing (DeepSeek)
   â†“
4. ğŸ”Š Text-to-Speech (Edge TTS)
   â†“
5. â–¶ï¸ Play Response
```

## ğŸ“‹ Prerequisites

Before running the application, ensure you have the following installed:

- **Python 3.12+**
- **Ollama**: Download and install from [ollama.com](https://ollama.com)
- **DeepSeek Model**: Pull the model using `ollama pull deepseek-v3.1:671b-cloud`
- **Groq API Key**: Get your API key from [groq.com](https://groq.com)

## ğŸš€ Installation

1. **Clone the repository**
   ```bash
   git clone <your-repository-url>
   cd Ai-voice-assistant
   ```

2. **Create a virtual environment**
   ```bash
   uv venv
   ```

3. **Activate the virtual environment**
   - Windows:
     ```bash
     .venv\Scripts\activate
     ```
   - macOS/Linux:
     ```bash
     source .venv/bin/activate
     ```

4. **Install dependencies**
   ```bash
   uv add -r requirements.txt
   ```

5. **Set up environment variables**
   
   Create a `.env` file in the root directory:
   ```env
   GROQ_API_KEY=your_groq_api_key_here
   ```

6. **Start Ollama service**
   ```bash
   ollama serve
   ```

7. **Pull the DeepSeek model**
   ```bash
   ollama pull deepseek-v3.1:671b-cloud
   ```

## ğŸ¯ Usage

1. **Run the application**
   ```bash
   streamlit run app.py
   ```

2. **Open your browser**
   
   The app will automatically open at `http://localhost:8501`

3. **Start using the voice assistant**
   - Click "ğŸ™ï¸ Start Recording" to record your question
   - Click "â¹ï¸ Stop Recording" when finished
   - Wait for the AI to process and respond
   - Listen to the audio response

## ğŸ“¦ Dependencies

- **streamlit**: Web application framework
- **streamlit-mic-recorder**: Audio recording widget
- **groq**: Groq API client for Whisper
- **python-dotenv**: Environment variable management
- **langchain**: LLM framework
- **langchain-ollama**: Ollama integration for LangChain
- **edge-tts**: Text-to-speech synthesis
- **sounddevice**: Audio recording
- **scipy**: Audio processing
- **playsound3**: Audio playback

## ğŸ—‚ï¸ Project Structure

```
Ai-voice-assistant/
â”œâ”€â”€ app.py                  # Main Streamlit application
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ pyproject.toml         # Project configuration
â”œâ”€â”€ .env                   # Environment variables (not in repo)
â”œâ”€â”€ README.md              # Project documentation
â”œâ”€â”€ human-text.txt         # Transcribed user input
â”œâ”€â”€ llm-text.txt          # LLM response text
â”œâ”€â”€ voice-output.wav      # Recorded audio file
â”œâ”€â”€ audio_output.mp3      # Generated speech response
â””â”€â”€ src/                  # Source modules
    â”œâ”€â”€ audio-recorder.py     # Audio recording module
    â”œâ”€â”€ speech-to-text.py     # STT module
    â”œâ”€â”€ llm.py                # LLM processing module
    â”œâ”€â”€ text-to-speech.py     # TTS module
    â””â”€â”€ play-audio.py         # Audio playback module
```

## âš™ï¸ Configuration

### Voice Settings

The default voice is set to `hi-IN-MadhurNeural` (Hindi voice). To change the voice:

1. Open [app.py](app.py)
2. Find the `VOICE` variable in the `text_to_speech_async` function
3. Replace with your preferred voice from [Edge TTS voices](https://speech.microsoft.com/portal/voicegallery)

### LLM Model

The default model is `deepseek-v3.1:671b-cloud`. To use a different model:

1. Pull the desired model: `ollama pull <model-name>`
2. Update the `model` parameter in the `process_with_llm` function

## ğŸ”§ Troubleshooting

### Ollama Connection Error

**Error**: "Make sure Ollama is running and the model is installed."

**Solution**:
```bash
# Start Ollama service
ollama serve

# Pull the model
ollama pull deepseek-v3.1:671b-cloud
```

### Microphone Access Issues

**Error**: Microphone not working

**Solution**:
- Ensure your browser has microphone permissions enabled
- Check system audio settings
- Try using a different browser (Chrome/Edge recommended)

### API Key Error

**Error**: "Invalid API key"

**Solution**:
- Verify your `.env` file contains the correct Groq API key
- Ensure the key is formatted as: `GROQ_API_KEY=your_key_here`
- Restart the application after updating the `.env` file

## ğŸ¨ Features in Detail

### Conversation History

- All conversations are stored in session state
- View previous interactions with timestamps
- Clear history with a single button click
- Expandable conversation cards for easy navigation

### Auto-play Audio

- Responses automatically play after generation
- Manual playback controls available
- Audio files saved locally for reference

## ğŸš€ Future Enhancements

- [ ] Support for multiple languages
- [ ] Custom voice selection UI
- [ ] Export conversation history
- [ ] Integration with more LLM providers
- [ ] Wake word detection
- [ ] Conversation context retention
- [ ] Voice customization options

## ğŸ“ License

This project is open source and available under the [MIT License](LICENSE).

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

For questions or support, please open an issue in the repository.

---

<div align="center">
  <strong>Powered by Groq, Whisper, Ollama, and Edge TTS</strong>
  <br>
  Made with â¤ï¸ using Streamlit
</div>


